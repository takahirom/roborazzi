name: Ollama Gemma Test

on:
  push:
    branches:
      - main
  pull_request:

env:
  OLLAMA_MODEL_TO_TEST: 'gemma3:27b-it-qat'
  OLLAMA_VOLUME_PATH: ollama_data

jobs:
  ollama-test:
    runs-on: ubuntu-latest
    permissions:
      contents: read

    steps:
      - name: Checkout repository code
        uses: actions/checkout@v4

      - name: Setup Gradle
        uses: gradle/gradle-build-action@a8f75513eafdebd8141bd1cd4e30fcd194af8dfa # v2.12.0
        with:
          gradle-version: wrapper

      - name: Restore Ollama Docker volume cache
        id: cache-ollama-volume
        uses: actions/cache@v4
        with:
          path: /var/lib/docker/volumes/${{ env.OLLAMA_VOLUME_PATH }}/_data
          key: ollama-docker-volume-${{ runner.os }}-${{ env.OLLAMA_MODEL_TO_TEST }}
          restore-keys: |
            ollama-docker-volume-${{ runner.os }}-

#      - name: Create Swap Memory
#        run: |
#          echo "Setting up swap memory..."
#          sudo fallocate -l 8G /swapfile
#          sudo chmod 600 /swapfile
#          sudo mkswap /swapfile
#          sudo swapon /swapfile
#          echo "/swapfile swap swap defaults 0 0" | sudo tee -a /etc/fstab
#          free -h

      - name: Create custom Modelfile
        run: |
          echo "Creating custom Modelfile for Gemma with reduced context window..."
          cat > Modelfile << EOF
          FROM gemma3:27b-it-qat

          PARAMETER num_ctx 4096
          PARAMETER temperature 0.1
          PARAMETER stop "<end_of_turn>"

          TEMPLATE """
          {{- range \$i, \$_ := .Messages }}
          {{- \$last := eq (len (slice \$.Messages \$i)) 1 }}
          {{- if or (eq .Role "user") (eq .Role "system") }}<start_of_turn>user
          {{ .Content }}<end_of_turn>
          {{ if \$last }}<start_of_turn>model
          {{ end }}
          {{- else if eq .Role "assistant" }}<start_of_turn>model
          {{ .Content }}{{ if not \$last }}<end_of_turn>
          {{ end }}
          {{- end }}
          {{- end }}
          """
          EOF
          cat Modelfile

      - name: Start Ollama using Docker
        run: |
          echo "Starting Ollama container using Docker..."
          docker run -d -p 11434:11434 \
            -v ${{ env.OLLAMA_VOLUME_PATH }}:/root/.ollama \
            -v $(pwd)/Modelfile:/Modelfile \
            -e OLLAMA_FLASH_ATTENTION=1 \
            --name ollama ollama/ollama:0.6.6-rc2

          echo "Waiting for Ollama service to initialize..."
          timeout=120
          interval=5
          elapsed=0
          while ! curl -sf http://localhost:11434/; do
            if [ $elapsed -ge $timeout ]; then
              echo "Ollama failed to start within $timeout seconds."
              echo "Docker ps output:"
              docker ps -a
              echo "Ollama container logs:"
              docker logs ollama
              exit 1
            fi
            echo "Ollama not ready yet, waiting $interval seconds..."
            sleep $interval
            elapsed=$((elapsed + interval))
          done
          echo "Ollama service is ready."

          echo "Checking running Docker containers..."
          docker ps
          curl --fail http://localhost:11434/ || (echo "Ollama server failed to start"; exit 1)

      - name: Pull Ollama Model and Create Custom Model
        run: |
          echo "Pulling base model: ${{ env.OLLAMA_MODEL_TO_TEST }}"
          docker exec ollama ollama pull ${{ env.OLLAMA_MODEL_TO_TEST }}
          
          echo "Creating custom model with reduced context window"
          docker exec ollama ollama create gemma3-reduced-ctx -f /Modelfile
          
          # Update the model to test
          echo "OLLAMA_MODEL_TO_TEST=gemma3-reduced-ctx" >> $GITHUB_ENV
          
          echo "Listing available models in Ollama..."
          docker exec ollama ollama list
          
          echo "Sending warm-up request..."
          docker exec ollama ollama run gemma3-reduced-ctx "Respond with just OK."
          echo "Warm-up complete."

      - name: Run Ollama Tests
        id: ollama_test
        continue-on-error: true
        run: |
          ./gradlew sample-android:testDebugUnitTest --tests "*OllamaWithOpenAiApiInterfaceTest"

      - name: Show warning on test failure
        if: steps.ollama_test.outcome != 'success'
        run: |
          echo "::warning::Ollama gemma3-reduced-ctx tests failed or encountered issues. This might be acceptable as the local LLM is not yet reliable. Please check the reports artifact for details."

      - uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4
        if: always()
        with:
          name: reports
          path: |
            **/build/reports
            **/build/outputs/roborazzi
          retention-days: 30
